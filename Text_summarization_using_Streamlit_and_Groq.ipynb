{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tapasmahanand/Text_summarization_using_Streamlit_and_Groq/blob/main/Text_summarization_using_Streamlit_and_Groq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYqF9EYensZF"
      },
      "outputs": [],
      "source": [
        "#Cell 1 - Install Required Libraries:\n",
        "!pip install streamlit PyMuPDF4LLM pandas openpyxl groq pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb1X6rG-qTm4"
      },
      "outputs": [],
      "source": [
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9sfsBbenscH"
      },
      "outputs": [],
      "source": [
        "#Cell 2 - Import Libraries and Set Up Environment:\n",
        "import streamlit as st\n",
        "import fitz  # PyMuPDF4LLM\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from groq import Groq\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUcxPPKpoKtk",
        "outputId": "f7d98801-76aa-46e7-e741-403076395d99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('groq_colab_key') #krch\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = userdata.get('NGROK_AUTH_TOKEN')\n",
        "\n",
        "# Get ngrok auth token from environment\n",
        "ngrok_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "!ngrok authtoken {ngrok_token}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import fitz\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from groq import Groq\n",
        "import tempfile\n",
        "import io\n",
        "\n",
        "# Initialize Groq client\n",
        "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "\n",
        "# Define models configuration\n",
        "MODEL_CONFIGS = {\n",
        "    \"model1\": {\n",
        "        \"name\": \"gemma2-9b-it\",\n",
        "        \"display_name\": \"Gemma2_9B\"\n",
        "    },\n",
        "    \"model2\": {\n",
        "        \"name\": \"llama-3.2-3b-preview\",\n",
        "        \"display_name\": \"LLaMA 3.2\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def chunk_text(text, chunk_size=4000):\n",
        "    \"\"\"Split text into chunks of approximately chunk_size words.\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_size = 0\n",
        "\n",
        "    for word in words:\n",
        "        current_chunk.append(word)\n",
        "        current_size += 1\n",
        "\n",
        "        if current_size >= chunk_size:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_size = 0\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text content from PDF file.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error processing PDF {pdf_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def get_summary_from_groq(text, model_name, max_words=None):\n",
        "    \"\"\"Get summary from Groq API with specified model and token management.\"\"\"\n",
        "    try:\n",
        "        # Split text into smaller chunks\n",
        "        chunks = chunk_text(text)\n",
        "        summaries = []\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            # Add delay for API rate limiting\n",
        "            time.sleep(10)\n",
        "\n",
        "            # Create appropriate prompt based on whether it's the main chunk\n",
        "            if len(chunks) > 1:\n",
        "                if i == 0:\n",
        "                    prompt = f\"This is part 1 of {len(chunks)} parts. \"\n",
        "                else:\n",
        "                    prompt = f\"This is part {i+1} of {len(chunks)} parts. \"\n",
        "\n",
        "                if max_words:\n",
        "                    prompt += f\"Please provide a brief summary of this part (the full summary across all parts should be under {max_words} words):\\n\\n{chunk}\"\n",
        "                else:\n",
        "                    prompt += f\"Please provide a concise summary of this part:\\n\\n{chunk}\"\n",
        "            else:\n",
        "                if max_words:\n",
        "                    prompt = f\"Please provide a summary of the following text in under {max_words} words:\\n\\n{chunk}\"\n",
        "                else:\n",
        "                    prompt = f\"Please provide a concise summary of the following text:\\n\\n{chunk}\"\n",
        "\n",
        "            try:\n",
        "                chat_completion = client.chat.completions.create(\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": prompt\n",
        "                        }\n",
        "                    ],\n",
        "                    model=model_name,\n",
        "                    temperature=0.7,\n",
        "                    max_tokens=500  # Limiting response tokens\n",
        "                )\n",
        "\n",
        "                summaries.append(chat_completion.choices[0].message.content)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error in chunk {i+1}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # If we have multiple summaries, combine them\n",
        "        if len(summaries) > 1:\n",
        "            combined_text = \" \".join(summaries)\n",
        "\n",
        "            # Add delay for API rate limiting\n",
        "            time.sleep(10)\n",
        "\n",
        "            # Get final summary of all chunks\n",
        "            final_prompt = f\"Please provide a {'concise' if not max_words else f'under {max_words} words'} summary combining these separate summaries:\\n\\n{combined_text}\"\n",
        "\n",
        "            chat_completion = client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": final_prompt\n",
        "                    }\n",
        "                ],\n",
        "                model=model_name,\n",
        "                temperature=0.7,\n",
        "                max_tokens=500\n",
        "            )\n",
        "\n",
        "            return chat_completion.choices[0].message.content\n",
        "        else:\n",
        "            return summaries[0] if summaries else None\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error getting summary from Groq API: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def process_pdfs(uploaded_files):\n",
        "    \"\"\"Process PDFs and generate summaries using both models.\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for uploaded_file in uploaded_files:\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
        "            tmp_file.write(uploaded_file.getvalue())\n",
        "            tmp_path = tmp_file.name\n",
        "\n",
        "        text = extract_text_from_pdf(tmp_path)\n",
        "        os.unlink(tmp_path)  # Clean up temporary file\n",
        "\n",
        "        if text:\n",
        "            result = {'PDF_Name': uploaded_file.name}\n",
        "\n",
        "            # Generate summaries for each model\n",
        "            for model_key, model_config in MODEL_CONFIGS.items():\n",
        "                model_name = model_config['name']\n",
        "                display_name = model_config['display_name']\n",
        "\n",
        "                # Get 50-word summary\n",
        "                summary_50 = get_summary_from_groq(text, model_name, max_words=50)\n",
        "                result[f'{display_name}_50_Words'] = summary_50\n",
        "\n",
        "                # Get unlimited summary\n",
        "                summary_unlimited = get_summary_from_groq(text, model_name)\n",
        "                result[f'{display_name}_Unlimited'] = summary_unlimited\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "def export_to_excel(df):\n",
        "    \"\"\"Export DataFrame to Excel file in memory.\"\"\"\n",
        "    output = io.BytesIO()\n",
        "    with pd.ExcelWriter(output, engine='openpyxl') as writer:\n",
        "        df.to_excel(writer, index=False)\n",
        "    output.seek(0)\n",
        "    return output\n",
        "\n",
        "def main():\n",
        "    st.title(\"LLM PDF Summarization Comparison\")\n",
        "\n",
        "    # Model configuration in sidebar\n",
        "    st.sidebar.title(\"Model Configuration\")\n",
        "    for model_key in MODEL_CONFIGS:\n",
        "        MODEL_CONFIGS[model_key]['name'] = st.sidebar.text_input(\n",
        "            f\"Enter {model_key} name\",\n",
        "            value=MODEL_CONFIGS[model_key]['name']\n",
        "        )\n",
        "        MODEL_CONFIGS[model_key]['display_name'] = st.sidebar.text_input(\n",
        "            f\"Enter {model_key} display name\",\n",
        "            value=MODEL_CONFIGS[model_key]['display_name']\n",
        "        )\n",
        "\n",
        "    # File uploader for PDFs\n",
        "    uploaded_files = st.file_uploader(\"Upload PDF files\", type=['pdf'], accept_multiple_files=True)\n",
        "\n",
        "    if uploaded_files:\n",
        "        if st.button(\"Generate Summaries\"):\n",
        "            with st.spinner(\"Processing PDFs...\"):\n",
        "                results = process_pdfs(uploaded_files)\n",
        "\n",
        "                # Display results\n",
        "                for result in results:\n",
        "                    st.subheader(f\"Results for {result['PDF_Name']}\")\n",
        "\n",
        "                    col1, col2 = st.columns(2)\n",
        "\n",
        "                    # Display results for each model in columns\n",
        "                    for i, (model_key, model_config) in enumerate(MODEL_CONFIGS.items()):\n",
        "                        with col1 if i == 0 else col2:\n",
        "                            display_name = model_config['display_name']\n",
        "                            st.write(f\"{display_name} Summaries:\")\n",
        "                            st.write(\"50 Words:\", result[f'{display_name}_50_Words'])\n",
        "                            st.write(\"Unlimited:\", result[f'{display_name}_Unlimited'])\n",
        "\n",
        "                # Create Excel file and provide download button\n",
        "                df = pd.DataFrame(results)\n",
        "                excel_file = export_to_excel(df)\n",
        "                st.download_button(\n",
        "                    label=\"Download Excel Report\",\n",
        "                    data=excel_file,\n",
        "                    file_name=\"summary_comparison.xlsx\",\n",
        "                    mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
        "                )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpmbvDssP5fQ",
        "outputId": "f1356604-ce55-4cff-fce3-9ae039c83a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXkDp8lvpLpl",
        "outputId": "06510519-568f-4a24-bc5c-b6c863c296cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app URL: https://3ee3-34-60-104-169.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Kill any existing Streamlit processes\n",
        "!kill -9 $(pgrep streamlit) 2>/dev/null\n",
        "\n",
        "# Start Streamlit\n",
        "!streamlit run app.py &>/content/logs.txt & #llama3.3 vs llama3.2\n",
        "# !streamlit run app_gemma.py &>/content/logs.txt & #gemma2-9b vs llama3.2\n",
        "time.sleep(5)\n",
        "\n",
        "# Create ngrok tunnel with correct configuration\n",
        "ngrok_tunnel = ngrok.connect(addr=\"8501\", proto=\"http\", bind_tls=True)\n",
        "print(f\"Streamlit app URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# Keep the tunnel open\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Closing ngrok tunnel...\")\n",
        "    ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSVBYKCBqxGL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}